{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b92ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "YKS Analyzer System - Main Notebook\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 1: IMPORT NECESSARY MODULES\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Custom Modules\n",
    "from data_loader import GoogleSheetsLoader\n",
    "from data_cleaner import DataCleaner\n",
    "from analysis.net_analyzer import NetAnalyzer\n",
    "from analysis.topic_analyzer import TopicAnalyzer\n",
    "from visualization.net_charts import NetVisualizer\n",
    "from visualization.topic_charts import TopicVisualizer\n",
    "from config import Config\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"âœ… All modules imported successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a811da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: Configuration and Initialization\n",
    "# ============================================================================\n",
    "\n",
    "# Import .env variables\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "config = Config()\n",
    "Config.validate_config()\n",
    "\n",
    "# print configuration summary\n",
    "SHEET_URL = os.getenv(\"GOOGLE_SHEET_URL\")\n",
    "CREDENTIALS_PATH = os.getenv(\"CREDENTIALS_PATH\")\n",
    "print(\"ðŸ“„ Google Sheets URL:\", SHEET_URL)\n",
    "print(\"ðŸ”‘ Credentials Path:\", CREDENTIALS_PATH)\n",
    "\n",
    "EXAM_TYPE = \"AYT\" #Select \"TYT\" or \"AYT\" here.\n",
    "\n",
    "TARGET_NET = config.Analysis.DEFAULT_TARGET_NET[EXAM_TYPE]\n",
    "\n",
    "print(f\"ðŸ“Š Exam Type: {EXAM_TYPE}\")\n",
    "print(f\"ðŸŽ¯ Target Net: {TARGET_NET}\")\n",
    "\n",
    "# Initialize main objects\n",
    "loader = GoogleSheetsLoader(SHEET_URL, CREDENTIALS_PATH)\n",
    "cleaner = DataCleaner(config, strict_mode=False, auto_fix=True)\n",
    "net_analyzer = NetAnalyzer(config, exam_type=EXAM_TYPE)\n",
    "topic_analyzer = TopicAnalyzer(config, exam_type=EXAM_TYPE)\n",
    "net_viz = NetVisualizer(config)\n",
    "topic_viz = TopicVisualizer(config)\n",
    "\n",
    "print(\"\\nâœ… All main objects are created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5992f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ“¥ Loading data...\")\n",
    "\n",
    "if EXAM_TYPE == \"TYT\":\n",
    "    raw_data = loader.load_tyt_data()\n",
    "else:\n",
    "    raw_data = loader.load_ayt_data()\n",
    "\n",
    "print(f\"âœ… {len(raw_data)} records loaded from Google Sheets.\")\n",
    "print(\"\\nðŸ“‹ First three rows:\")\n",
    "display(raw_data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e886b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: Data Cleaning\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸ§¹ Cleaning Data...\")\n",
    "\n",
    "# Create cleaner object\n",
    "cleaner = DataCleaner(config, strict_mode=False, auto_fix=True)\n",
    "\n",
    "# Clean full dataset\n",
    "cleaned_data = cleaner.clean_full_dataset(raw_data, exam_type=EXAM_TYPE)\n",
    "\n",
    "# Add derived features\n",
    "cleaned_data = cleaner.add_derived_features(cleaned_data, exam_type=EXAM_TYPE)\n",
    "\n",
    "# Show cleaning report\n",
    "report = cleaner.get_cleaning_report()\n",
    "print(f\"\\nðŸ“Š Cleaning Raporu:\")\n",
    "print(f\"  â€¢ Deleted rows: {report['rows_removed']}\")\n",
    "print(f\"  â€¢ Fixed values: {report['values_fixed']}\")\n",
    "print(f\"  â€¢ Warnings: {len(report['warnings'])}\")\n",
    "\n",
    "print(f\"\\nâœ… Cleaning completeÄ±: {len(cleaned_data)} Exam(s)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500e9621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: Net Analysis\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸ“Š Analyzing nets...\")\n",
    "\n",
    "# Create analyzer object\n",
    "net_analyzer = NetAnalyzer(config, exam_type=EXAM_TYPE)\n",
    "\n",
    "# Total net statistics\n",
    "print(\"\\n=== TOTAL NET STATISTICS ===\")\n",
    "total_stats = net_analyzer.calculate_statistics(cleaned_data, 'Toplam Net')\n",
    "for key, value in total_stats.items():\n",
    "    print(f\"  {key}: {value:.2f}\" if isinstance(value, float) else f\"  {key}: {value}\")\n",
    "\n",
    "# Trend analysis\n",
    "print(\"\\n=== TREND ANALYSIS ===\")\n",
    "trend = net_analyzer.get_progression_trend(cleaned_data, 'Toplam Net')\n",
    "print(f\"  Trend: {trend['trend']}\")\n",
    "print(f\"  Slope: {trend['slope']:.2f} net/deneme\")\n",
    "print(f\"  RÂ²: {trend['r_squared']:.3f}\")\n",
    "print(f\"  Total improvement: {trend['total_improvement']:.1f} net\")\n",
    "print(f\"  Next prediction: {trend['next_prediction']:.1f} net\")\n",
    "\n",
    "# All subjects statistics\n",
    "print(\"\\n=== SUBJECT BASED STATISTICS ===\")\n",
    "all_stats = net_analyzer.get_all_subjects_statistics(cleaned_data)\n",
    "print(all_stats[['mean', 'std', 'min', 'max']].round(2))\n",
    "\n",
    "# Weak and strong subjects\n",
    "print(\"\\n=== WEAK SUBJECTS ===\")\n",
    "weak = net_analyzer.identify_weak_subjects(cleaned_data)\n",
    "for subject_info in weak[:3]:\n",
    "    print(f\"  â€¢ {subject_info['subject']}: {subject_info['mean']:.1f} net (Trend: {subject_info['trend']})\")\n",
    "\n",
    "print(\"\\n=== STRONG SUBJECTS ===\")\n",
    "strong = net_analyzer.identify_strong_subjects(cleaned_data)\n",
    "for subject_info in strong[:3]:\n",
    "    print(f\"  â€¢ {subject_info['subject']}: {subject_info['mean']:.1f} net (Trend: {subject_info['trend']})\")\n",
    "\n",
    "# Comparison to target\n",
    "print(\"\\n=== COMPARISON TO TARGET ===\")\n",
    "target_comparison = net_analyzer.compare_to_target(cleaned_data, TARGET_NET)\n",
    "print(f\"  Target: {target_comparison['target']}\")\n",
    "print(f\"  Current: {target_comparison['current']:.1f}\")\n",
    "print(f\"  Gap: {target_comparison['gap']:.1f} net\")\n",
    "print(f\"  Status: {target_comparison['status']}\")\n",
    "if target_comparison['exams_needed']:\n",
    "    print(f\"  Expected exam count: {target_comparison['exams_needed']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd051fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: Subject Analysis\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸ“š Subject analysis being made...\")\n",
    "\n",
    "\n",
    "topic_analyzer = TopicAnalyzer(config, exam_type=EXAM_TYPE)\n",
    "# precomputation step\n",
    "precomputed_topic_trends = topic_analyzer._precompute_all_topic_trends(cleaned_data)\n",
    "\n",
    "\n",
    "# Most problematic topics\n",
    "print(\"\\n=== MOST PROBLEMATIC 10 TOPIC ===\")\n",
    "problematic = topic_analyzer.get_most_problematic_topics(cleaned_data, top_n=10)\n",
    "for i, (topic, count) in enumerate(problematic, 1):\n",
    "    print(f\"  {i}. {topic}: {count} times\")\n",
    "\n",
    "# Weak areas (subject based)\n",
    "print(\"\\n=== WEAK AREAS (SUBJECT BASED) ===\")\n",
    "weak_areas = topic_analyzer.identify_weak_areas(cleaned_data, threshold=3)\n",
    "for subject, topics in weak_areas.items():\n",
    "    print(f\"\\n{subject}:\")\n",
    "    for topic in topics[:2]:  # Show top 2 weak topics per subject\n",
    "        print(f\"  â€¢ {topic}\")\n",
    "\n",
    "\n",
    "# Study plan\n",
    "print(\"\\n=== SUGGESTED STUDY PLAN ===\")\n",
    "study_plan = topic_analyzer.generate_study_plan(\n",
    "    cleaned_data, \n",
    "    precomputed_topic_trends, \n",
    "    max_topics_per_subject=3\n",
    ")\n",
    "for subject, plan in study_plan.items():\n",
    "    print(f\"\\n{subject}:\")\n",
    "    for item in plan:\n",
    "        print(f\"  {item['order']}. {item['topic']}\")\n",
    "        print(f\"     Priority: {item['priority']} | Frequency: {item['frequency']} | {item['recent_status']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc7ce8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: Net Graphics\n",
    "# ============================================================================\n",
    "\n",
    "# Net Analysis Results\n",
    "all_net_stats = net_analyzer.get_all_subjects_statistics(cleaned_data)\n",
    "improvement_data = net_analyzer.calculate_improvement_rate(cleaned_data, window=3)\n",
    "\n",
    "# Topic Analysis Results\n",
    "most_problematic = topic_analyzer.get_most_problematic_topics(cleaned_data, top_n=15)\n",
    "subject_comparison_topics = topic_analyzer.compare_subjects_by_topics(cleaned_data)\n",
    "\n",
    "\n",
    "print(\"\\nðŸ“ˆ Generating net graphics...\")\n",
    "\n",
    "# Create visualizer object\n",
    "net_viz = NetVisualizer(config)\n",
    "\n",
    "# Create charts directory if not exists\n",
    "charts_dir = Path(\"output/charts\")\n",
    "charts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Total net trend\n",
    "fig1 = net_viz.plot_total_nets_by_exam(\n",
    "    cleaned_data, \n",
    "    save_path=charts_dir / \"total_net.png\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Comparison of all subjects (last exam)\n",
    "fig2 = net_viz.plot_all_subjects_comparison(\n",
    "    cleaned_data,\n",
    "    save_path=charts_dir / \"subject_comparison.png\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "net_viz.dashboard(cleaned_data, exam_type=EXAM_TYPE)\n",
    "\n",
    "net_columns = [col for col in cleaned_data.columns if col.endswith('Net')]\n",
    "\n",
    "# Comparison of all subjects (multi-line)\n",
    "fig4 = net_viz.plot_multi_subject_comparison(\n",
    "    cleaned_data,\n",
    "    net_columns[:4] if len(net_columns) >= 4 else net_columns,\n",
    "    save_path=charts_dir / \"all_subjects_multi_comparison.png\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ… Saved bet graphics: {charts_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a2a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: Topic Graphics\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸ“Š Generating topic graphics...\")\n",
    "\n",
    "# Create visualizer object\n",
    "topic_viz = TopicVisualizer(config)\n",
    "\n",
    "# Most problematic topics in the last 3 exams\n",
    "last3_data = cleaned_data.sort_values('Tarih').tail(3)\n",
    "problematic_last3 = topic_analyzer.get_most_problematic_topics(last3_data, top_n=15)\n",
    "\n",
    "fig8 = topic_viz.plot_total_wrong_topics(\n",
    "    problematic_topics=problematic_last3, \n",
    "    top_n=15,\n",
    "    save_path=charts_dir / \"most_problematic_topics.png\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig = topic_viz.plot_topic_trend_by_exam(cleaned_data, 'Matematik', 'tÃ¼rev')\n",
    "if fig:\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enaough data for the graphic or the names doesnt match.\")\n",
    "\n",
    "\n",
    "print(f\"âœ… Saved subject graphics: {charts_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cf58ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: Final Summary Report\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸ“Š Generating final summary report...\")\n",
    "net_report = net_analyzer.generate_summary_report(cleaned_data)\n",
    "topic_report = topic_analyzer.generate_topic_summary_report(cleaned_data)\n",
    "\n",
    "print(f\"\\nðŸ“Š General Information:\")\n",
    "print(f\"  â€¢ Exam Type: {EXAM_TYPE}\")\n",
    "print(f\"  â€¢ Total Exam Count: {net_report.get('total_exams', 'N/A')}\")\n",
    "print(f\"  â€¢ Date Range: {net_report.get('date_range', {}).get('first', pd.NaT).strftime('%Y-%m-%d')} -> {net_report.get('date_range', {}).get('last', pd.NaT).strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Net Performance:\")\n",
    "print(f\"  â€¢ Mean of Total Net: {net_report['overall_stats'].get('mean', 0):.2f}\")\n",
    "print(f\"  â€¢ Highest Total Net: {net_report['overall_stats'].get('max', 0):.2f}\")\n",
    "print(f\"  â€¢ Latest Exam Total Net: {net_report['overall_stats'].get('latest', 0):.2f}\")\n",
    "print(f\"  â€¢ Improvement: {net_report['recent_improvement'].get('interpretation', 'N/A')}\")\n",
    "\n",
    "\n",
    "print(\"\\nðŸŽ‰ Analysis Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37c2ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 10: Save data (Optional)\n",
    "# ============================================================================\n",
    "\n",
    "# Save cleaned data as csv\n",
    "output_data_dir = Path(\"output/data\")\n",
    "output_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cleaned_data.to_csv(output_data_dir / f\"{EXAM_TYPE}_cleaned_data.csv\", index=False)\n",
    "print(f\"\\nðŸ’¾ Cleaned data saved successfully: {output_data_dir / f'{EXAM_TYPE}_cleaned_data.csv'}\")\n",
    "\n",
    "# Save summary reports as JSON\n",
    "import json\n",
    "\n",
    "with open(output_data_dir / f\"{EXAM_TYPE}_net_report.json\", 'w', encoding='utf-8') as f:\n",
    "    # Convert DataFrames to dicts\n",
    "    report_to_save = net_report.copy()\n",
    "    for key in report_to_save:\n",
    "        if isinstance(report_to_save[key], pd.DataFrame):\n",
    "            report_to_save[key] = report_to_save[key].to_dict()\n",
    "    json.dump(report_to_save, f, ensure_ascii=False, indent=2, default=str)\n",
    "\n",
    "print(f\"ðŸ’¾ Net report saved successfully: {output_data_dir / f'{EXAM_TYPE}_net_report.json'}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ All processes completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7bf271",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
